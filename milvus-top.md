# milvus 设计方案
1. 所有修改操作(插入、删除、合并)均生成一个新的文件，原文件并不直接删除，而是由后台程序安全的删除
2. `etcd` 的每次修改都会导致 `revision` 值加 `1`，并且可以查询 `key` 在指定 `revision` 下的值
3. 只有对 `etcd` 中的 `meta` 修改完成后，本次的修改操作才算成功
4. 使用前 3 条实现 `MVCC` 多版本控制
5. 一个 `flush` 语句块对应一个文件，可以包含多个向量，不是一条向量一个文件
6. 仿照 `ES` 使用 `replica` 实现 `HA`

---

## `etcd` 的 `revision`
```bash
$ etcdctl --endpoints=http://127.0.0.1:10001 put k1 v1 -w json
{"header":{"cluster_id":18293669711776909085,"member_id":8241799522139745222,"revision":2,"raft_term":2}}
$ etcdctl --endpoints=http://127.0.0.1:10001 put k2 v2 -w json
{"header":{"cluster_id":18293669711776909085,"member_id":8241799522139745222,"revision":3,"raft_term":2}}
$ etcdctl --endpoints=http://127.0.0.1:10001 put k3 v3 -w json
{"header":{"cluster_id":18293669711776909085,"member_id":8241799522139745222,"revision":4,"raft_term":2}}
$ etcdctl --endpoints=http://127.0.0.1:10001 put k4 v4 -w json
{"header":{"cluster_id":18293669711776909085,"member_id":8241799522139745222,"revision":5,"raft_term":2}}
$ etcdctl --endpoints=http://127.0.0.1:10001 put k5 v5 -w json
{"header":{"cluster_id":18293669711776909085,"member_id":8241799522139745222,"revision":6,"raft_term":2}}
$ etcdctl --endpoints=http://127.0.0.1:10001 get k --prefix
k1
v1
k2
v2
k3
v3
k4
v4
k5
v5
$ etcdctl --endpoints=http://127.0.0.1:10001 del k5 -w json
{"header":{"cluster_id":18293669711776909085,"member_id":8241799522139745222,"revision":7,"raft_term":2},"deleted":1}
$ etcdctl --endpoints=http://127.0.0.1:10001 get k --prefix
k1
v1
k2
v2
k3
v3
k4
v4
$ etcdctl --endpoints=http://127.0.0.1:10001 get k --prefix --rev 6
k1
v1
k2
v2
k3
v3
k4
v4
k5
v5
$ etcdctl --endpoints=http://127.0.0.1:10001 put k2 v22 -w json
{"header":{"cluster_id":18293669711776909085,"member_id":8241799522139745222,"revision":8,"raft_term":2}}
$ etcdctl --endpoints=http://127.0.0.1:10001 get k2
k2
v22
$ etcdctl --endpoints=http://127.0.0.1:10001 get k2 --rev 7
k2
v2
$ etcdctl --endpoints=http://127.0.0.1:10001 get k --prefix --rev 7
k1
v1
k2
v2
k3
v3
k4
v4

```

---


## 前提假设
- 同一个客户端严格保操作时序，不同客户端不保证时序
    - C<sub>1</sub> 在本地时刻 T<sub>0</sub> 发起插入操作 I<sub>0</sub>，插入数据 D<sub>A</sub>，并执行 `flush` 操作
    - C<sub>1</sub> 在本地时刻T<sub>1</sub> 发起查询操作 Q<sub>1</sub>，数据 D<sub>A</sub>一定对Q<sub>1</sub>可见
    - C<sub>2</sub> 在本地时刻T<sub>2</sub>时刻发起查询操作 Q<sub>2</sub>，数据 D<sub>A</sub>不保证对Q<sub>2</sub>可见
    - 因为 I<sub>0</sub> 与 Q<sub>1</sub> 都是由 C<sub>1</sub> 发起，I<sub>0</sub> 与 Q<sub>1</sub>属于同步操作，所以 Q<sub>1</sub> 命令到达服务端时，I<sub>0</sub>操作已经完成 ，所以D<sub>A</sub>一定对Q<sub>1</sub>可见
    - Q<sub>2</sub> 和  I<sub>0</sub> 属于不同客户端发起的操作,所以不能保证 Q<sub>2</sub> 达到服务器后， I<sub>0</sub> 操作已经完成，数据 D<sub>A</sub>不保证对Q<sub>2</sub>可见
    - 如果 C<sub>1</sub> 和 C<sub>2</sub> 是由同一个用户发起的的链接，并且用户确保 I<sub>0</sub> 操作返回之后，再发起 Q<sub>2</sub> 操作，那么数据 D<sub>A</sub>保证对Q<sub>2</sub>可见
- 未 `flush` 之前，数据对所有客户端均不可见，`flush` 之后数据对所有客户端可见
  - C<sub>1</sub> 在本地时刻 T<sub>0</sub> 发起插入操作 I<sub>0</sub>，插入数据 D<sub>A</sub>，未执行 `flush` 操作
  - C<sub>1</sub> 在本地时刻 T<sub>1</sub> 发起查询操作 Q<sub>1</sub>，数据 D<sub>A</sub>对Q<sub>1</sub>不可见
  - C<sub>1</sub> 在本地时刻 T<sub>2</sub> 发起 `flush` 操作
  - C<sub>1</sub> 在本地时刻 T<sub>3</sub> 发起查询操作 Q<sub>3</sub>，数据 D<sub>A</sub>对Q<sub>3</sub>可见
- 未 `flush` 的操作在客户端断开后均被抛弃
  - C<sub>1</sub> 在本地时刻 T<sub>0</sub> 发起插入操作 I<sub>0</sub>，插入数据 D<sub>A</sub>，未执行 `flush` 操作
  - C<sub>1</sub> 在本地时刻 T<sub>1</sub> 发起删除操作 D<sub>1</sub>，删除数据 D<sub>B</sub>，未执行 `flush` 操作
  - C<sub>1</sub> 在本地时刻 T<sub>2</sub> 与服务端断开链接
  - 数据 D<sub>A</sub> 在服务器端被抛弃，数据 D<sub>B</sub>在服务端未被删除，依然可以被查询到
- 只有 `insert` 和 `delete` 需要 `flush`，其它不需要,如 `create collection`,`drop collection`
- 同一个 `flush` 块内必须为同一种操作，不能混搭
  - 假设 `flush` 语句块为 : `{insert v1 into t1; insert v2 into t1; delete v3 from t1}`
  - 因为这个 `flush` 语句块同时包含 `insert 和  delete`，所以本次 `flush` 操作返回失败
- 同一个 `flush` 块内必须为同一个`collection`的操作，不能混搭
  - 假设 `flush` 语句块为 : `{insert v1 into t1; insert v2 into t2; insert v3 into t1}`
  - 因为这个 `flush` 语句块同时向 `t1` 和 `t2` 插入数据，所以本次 `flush` 操作失败
- 不涉及主键唯一性检查(待讨论)
- `delete` 指令中，被删除的数据不存在，删除操作依然成功，不返回错误信息，仅仅有日志记录了这个行为
- 支持粗粒度的`多写`操作
  - 一个 `client` 只有一个写节点，不存在把一次写操作的数据分成多份，然后多个写节点同时写的情况
  - 如果需要提高写入速度，可以链接多个 `client`
- 这个设计针对批插入，并且删除属于低频操作
- 不涉及用户管理，及访问控制，访问控制由 `IP` 地址白名单实现

---

## 配置文件
- `auto_flush`:
  - 类型 : `bool`
  - 默认值 : `false`
  - 无论是否设置 `auto_flush` , `milvus` 程序保证 `flush` 间的数据要么整体成功，要么整体失败
  - 当 `auto_flush` 为 `false` 时，`client` 端需要手动调用 `flush`
  - 当 `auto_flush` 为 `true` 时，以下两种情况均可以触发 `flush`
    - 定时超过 `flush_interval` 后触发
    - 插入的数据超过 `flush_limitition` 后触发
  - 因为同一个 `flush` 块内不能混搭不同的操作，也不能混搭不同的 `collection`，所以 `auto_flush` 需要自动划分 `flush` 块
  - 当 `auto_flush` 为 `true` 时，如果自动触发的 `flush` 操作运行失败，则数据丢失，不向用户返回任何信息，仅仅记录日志 
- `flush_interval`
  - 类型 : `int32`
  - 默认值 ： `1000`
  - 定时触发 `flush` 的时间间隔，单位为毫秒
- `flush_limitition`
  - 类型 : `string`
  - 默认值 : `16M`
  - 插入的数据超过 `flush_limitition` 后触发 `flush` 操作
- `num_nodes`:
  - 类型 : `int32`
  - 默认值 : `1`
  - 负责数据插入及查询节点的数目
- `num_replicas`:
  - 类型 : `int32`
  - 默认值 : `0`
  - 数据在不同节点间备份的数目,`0` 表示数据没有备份
  - 数据只能在不同的节点间备份，因此该值的上限为 `num_nodes-1`
- `sync_level`:
  - 类型 : `int32`
  - 默认值 : `0`
  - 同步等级，数据插入后，至少产生 `sync_level` 后再向 `client` 返回
  - 该值上限为 `num_replicas` 
- `master_address`: 
  - 无默认值
  - `Master` 节点的 `ip` 地址和端口
- `node_address_lists`:
  - 无默认值
  - 这是个列表，包含每个记录包含一下信息
    - `node` 节点编号,从 `0` 开始
    - `IP` 地址
    - 端口
- `high_water_level`:
  - 类型 : `double`
  - 默认值 : `0.6`
  - 取值范围为 `[0 1]` 当 `Milvus` 节点内存使用率达到 `hight_water_level` 后，表示当前节点已经没有富余内存，此时向节点插入数据会别转发到其他节点上
- `master_heart_beat_interval`:
  - 类型 : `int32`
  - 默认值 : `1000`
  - `Master` 节点向 `Milvus` 节点发送心跳信号请求的时间间隔
- `time_out_on_heart_beat`:
  - 类型 : `int32`
  - 默认值 ： `3`
  - 连续 `time_out_on_heart_beat` 次心跳信号返回超时，则认为对应的 `Milvus` 节点宕机
  - 在 `k8s` 环境中， 因为 `k8s` 可以帮助重启节点，所以该值可以设的比较大，因为认为失效的这段时间内，`Milvus` 节点可能正在重启过程中

### 和 `HA` 相关的配置项目
- num_nodes
- num_replicas
- sync_level

---

  
## 整体框图
```txt
+----------------------------------------------------------------------------------------+
|                                                               etcd                     |
+----------------------------------------------------------------------------------------+

+------------------------------------------+  +------------------------------------------+
|               Milvus-Cluster             |  |               Milvus-Cluster             |
|  +------------------------------------+  |  |  +------------------------------------+  |
|  |             Master                 |  |  |  |             Master                 |  |
|  +------------------------------------+  |  |  +------------------------------------+  |
|                                          |  |                                          |
|  +----------+ +----------+ +----------+  |  |  +----------+ +----------+ +----------+  |
|  |  Milvus  | |  Milvus  | |  Milvus  |  |  |  |  Milvus  | |  Milvus  | |  Milvus  |  |
|  +----------+ +----------+ +----------+  |  |  +----------+ +----------+ +----------+  |
|                                          |  |                                          |
+------------------------------------------+  +------------------------------------------+
```
- `Milvus-Cluster` 包含多个 `Miluvs` 节点
- `Milvus-Cluster` 有一个全局唯一的`ID`
- 学 `ES` 使用 `replica` 实现 `HA`，同一个 `fragement` 的数据在多个 `milvus` 节点上都有备份 

---

## 逻辑存储格式
```txt
+-----------------------------------------------------------------------------------------------------------------+
|                        collection                                                                               |
|                                                                                                                 |
|  +----------------------------------------------------------------------------------+                           |
|  |                    fragment 1  @                                                 |                           |
|  |  +------------+ +----------------+ +------------+ +------------+ +------------+  |                           |
|  |  | * raw file | | * delete log @ | | * column 1 | | * column 2 | | * column N |  |                           |
|  |  +------------+ +----------------+ +------------+ +------------+ +------------+  |                           |
|  +----------------------------------------------------------------------------------+                           |
|                                                                                                                 |
|  +-----------------------------------------------------------------------------------------------------------+  |
|  |                    fragment 2  @                                                                          |  |
|  |  +--------------+ +--------------+ +----------------+ +--------------+ +--------------+ +--------------+  |  |
|  |  | * index file | | * raw file 1 | | * delete log @ | | * column 1.1 | | * column 2.1 | | * column N.1 |  |  |
|  |  |              | +--------------+ |                | +--------------+ +--------------+ +--------------+  |  |
|  |  |              | +--------------+ |                | +--------------+ +--------------+ +--------------+  |  |
|  |  |              | | * raw file 2 | |                | | * column 1.2 | | * column 2.2 | | * column N.2 |  |  |
|  |  |              | +--------------+ |                | +--------------+ +--------------+ +--------------+  |  |
|  |  |              | +--------------+ |                | +--------------+ +--------------+ +--------------+  |  |
|  |  |              | | * raw file N | |                | | * column 1.N | | * column 2.N | | * column N.N |  |  |
|  |  +--------------+ +--------------+ +----------------+ +--------------+ +--------------+ +--------------+  |  |
|  +-----------------------------------------------------------------------------------------------------------+  |
|                                                                                                                 |
|  +---------------+                                                                                              |
|  | fragment 3  @ |                                                                                              |
|  +---------------+                                                                                              |
|                                                                                                                 |
|  +---------------+                                                                                              |
|  | fragment N  @ |                                                                                              |
|  +---------------+                                                                                              |
|                                                                                                                 |
+-----------------------------------------------------------------------------------------------------------------+
```

- 带 `*` 的方框对应一个`Milvus`节点上的文件，该文件可能存在 `replica`
- 带 `@` 的方框对应一条 `etcd` 的 `meta`
- `fragment 1`
  - 未建立索引，仅包含原始向量，而且只能有一个原始向量文件
  -  `raw file` 为原始向量文件
  - `delete log` 记录 `raw file` 被删除向量的偏移,从 `0` 开始
  - `column 1, column 2, column N` 为标量文件
  - `raw file, column 1, column 2, column N` 按照行对齐
- `fragemtn 2`
  - 已建立索引，只能包含一个索引文件，但是可以包含多个原始向量文件
  - `index file` 为索引文件，对应的原始向量为 `raw file 1, raw file 2, raw file 3` 的合计
  - `delete log` 记录 `index file` 被删除向量的偏移，从 `0` 开始
  - `raw file N, column 1.N, column 2.N, column N.N` 按照行对齐
- `fragment 2` 由多个类似 `fragment 1` 的集合合并完成，合并操作由 `Milvus` 后台自动完成
- 一次只包含多个 `insert` 的 `flush` 操作，生成一个 `fragment`
- 一个 `fragment` 只包含一个 `delete log` 文件
- 每个 `fragment` 有 `fragment id`， 类型为 `uint64`， 在 `collection` 内唯一

---

## 物理存储格式
```txt

+-----------------------------------------------------------------------------+
|                              Milvus-Cluster                                 |
|  +---------------------+  +---------------------+  +---------------------+  |
|  |     Milvus (N1)     |  |     Milvus (N2)     |  |     Milvus (N3)     |  |
|  |  +---------------+  |  |  +---------------+  |  |  +---------------+  |  |
|  |  | fragment (F1) |  |  |  | fragment (F0) |  |  |  | fragment (F0) |  |  |
|  |  +---------------+  |  |  +---------------+  |  |  +---------------+  |  |
|  |  +---------------+  |  |  +---------------+  |  |  +---------------+  |  |
|  |  | fragment (F2) |  |  |  | fragment (F1) |  |  |  | fragment (F2) |  |  |
|  |  +---------------+  |  |  +---------------+  |  |  +---------------+  |  |
|  +---------------------+  +---------------------+  +---------------------+  |
+-----------------------------------------------------------------------------+

```
- `collection` `T0` 包含三个 `fragment` `F0 F1 F2`
- `num_replicas` 设置为 `1`， 则每个 `fragment` 都有一个备份
- `N1` 节点包含 `F1 F2`
- `N2` 节点包含 `F0 F1`
- `N3` 节点包含 `F0 F2`

---

## `Milvus` 节点
- `$MILVUS_ROOT`
  - 这是个环境变量，指示数据在 `milvus` 节点上的存储路径
- 每个 `Milvus` 节点都可以从 `etcd` 获得完整的 `meta` 信息，能够知道当前查询指令包含几个文件，这些文件分别有几个 `replica` 以及分布在哪些 `Milvus` 节点上
- `Milvus` 节点具有唯一编号，从 `1` 开始递增
- 对外提供查询服务，获得 `Milvus` 节点正在执行的任务信息，包括
  - 查询操作
  - 插入操作
  - 二级查询操作
  - 复制操作， 包含数据源，文件名，etcd中的 `key` 和 `revision`
  - 创建索引操作
- 对外提供查询服务，指定的文件名是否在 `Milvus` 节点内，返回内容包括:
  - 文件名
  - 文件是否在节点的内存中
  - 文件是否在节点的磁盘中
- 对外提供查询服务，给定向量主键`id1` 和向量文件`F0`，返回内容包括:
  - 如果 `id1` 在 `F0` 内，则返回 `offset`
  - 如果不存在，则返回 `-1`
  - 该服务服务用于删除操作
- 对外提供查询服务，返回当前 `Milvus` 节点上所有正在执行操作的 `etcd revision` 值
  - 该服务用于索引文件合并后，原索引文件删除
- 定时任务
  - 根据 `meta` 定时检查是否存在与本节点相关的复制任务
    - 如果有复制任务，则从 `InsertNode` 复制文件
    - 因为 `复制操作` 为 `幂等` 行为，所以重复执行不影响最终结果
  - 文件合并操作并不直接删除原始的小文件，因为那些小文件可能正在被某个查询使用，需要定时任务删除无用的小文件

---

## `Master` 节点
- `Master` 是无状态的服务
- `Master` 根据负载状态告知 `client` 链接哪个 `Milvus` 节点
  - `client` 首先链接 `Master`,`Master`根据各个`Miluvs`节点的负载状态，选择一个合适的 `Miluvs`节点，并其 `IP` 地址和端口返回给 `client`
  - `client` 收到 `Master` 返回的 `Miluvs` 节点 `IP`地址和端口后，主动断开和 `Master` 的链接
  - `client` 链接目标的 `Milvus` 节点
  - 因此 `Milus-Cluster` 内的所有 `Milvus` 节点和 `Master` 节点都必须对外暴露自己的IP地址
```txt
              client 链接 Milvus 节点

+--------+                                   +--------+
|        |                                   |        |
|        | (1) Connect with Master           |        |
|        |----------------------------------->        |
|        |                                   |        |
|        | (2) Milvus node IP and Port       |        |
|        <-----------------------------------| Master |
|        |                                   |        |
|        | (3) Close connection with Master  |        |
| client <----------------X------------------>        |
|        |                                   |        |
|        |                                   |        |
|        |                                   |        |
|        |                                   +--------+
|        |
|        |
|        |                                   +--------+
|        | (4) Connect with Milvus           |        |
|        |-----------------------------------> Milvus |
|        |                                   |        |
+--------+                                   +--------+
```

- `Master` 定期的向所有的 `milvus` 节点请求心跳信号，确定其是否依然存活，如果节点宕机，例如 `N1`，则修改数据的 `replicas` 属性，将 `InsertNode` 从 `N1` 改为其它 `ReplicaNode` 节点
- `Master` 需要 `watch etcd` 的 这个 `/<user_name>/config/<Milvus-Cluster_ID>`，当有新的节点加入后会更新这个 `key`

---

## `etcd` 及元数据
- `etcd` 保存全局的 `meta` 信息
- 多个 `Milvus-Cluster` 可以共用一个 `etcd`
- `etcd` 中的 `meta` 类型
  - 用户列表
    - `key` 为 `/user_list`
    - `value` 为一个 `array`，每条记录包含一个`<user_name>`
  - `key` 对应一个 `delete log` 文件
    - `key` 命名格式 : `/<user_name>/<collection_name>/delete_log/<fragment_id>`
    - `value` 内容：
      - `delete log` 对应的文件路径,`$MILVUS_ROOT`下的相对路径
      - `replicas` : 一个列表，记录该文件的 `InsertNode` 和 `ReplicaNode`
  - `key` 对应一个 `fragment`
    - `key` 命名格式 : `/<user_name>/<collection_name>/data/<fragment_id>`
    - `value` 内容:
      - 索引类型 : `None` 或具体的索引类型；`None` 表示该 `fragment` 对应的原始向量尚未建立索引
      - `replicas` : 一个列表，记录该文件的 `InsertNode` 和 `ReplicaNode`
      - 按照 [逻辑存储格式](##逻辑存储格式) 所显示的`fragment`内各个文件的布局
      - 当前 `fragment` 总行数，以及总字节数目
      - 当前 `fragment` 向量主键的最大值和最小值
  - `key` 对应 `collection` 的基本信息
    - `key` 命名格式 : `/<user_name>/<collection_name>/schema`
    - `value`:
      - `schema`
      - 索引类型
      - `next_fragment_id` :
        - 类型 : `uint64`
        - 初始值 : 0
        - `Milvus` 节点每次以原子操作的方式加 `1` 获得一个 `fragment id`
      - `num_fragments` : `fragment` 总数
      - `num_rows` : 总行数，插入数据的总行数，包含已经删除的数据；如果需要排查删除的数据，用户需要根据 `delete log` 自行计算
  - `key` 对应属于当前用户的 `collection list`
    - `key` 命名格式为：`/<user_name>/collection_list`
    - `value` 为一个 `array`,每条记录的格式为: `<collection_name> : <create_time>, <index_type>` 按照 `<create_time>` 降序排列
  - `key` 对应属于当前用户的 `Milvus-Cluster` 属性，一个用户可以拥有多个 `Milvus-Cluster`
    - `key` 命名格式为：`/<user_name>/config/<Milvus-Cluster_ID>`
    - `value` 内容为 `json` 格式存储的 `Milvus-Cluster` 配置文件

### 关于 `replicas` 属性的说明
- 负责插入文件的节点称为该文件的 `InsertNode`
- 其它拥有该文件 `replica` 的节点称为该文件的 `ReplicaNode`
- `/<user_name>/<collection_name>/data/<fragment_id>` 和 `/<user_name>/<collection_name>/delete_log/<fragment_id>` 拥有 `replicas` 属性
- `replicas` 为一个列表，第一个元素为 `InsertNode`，后续元素为 `ReplicaNode`
- 在 `ReplicaNode` 前添加 “ `-` ” 符号表示文件正在往节点复制的过程中
- `[2,1,5]` 表示该文件是由节点 `2` 插入的，并且在节点 `1` 和 节点`5` 上拥有备份，并且这两个备份在本次查询中是可用的
- `[2,-1,5]` 表示该文件是由节点 `2` 插入的，并且在节点 `1` 和 节点`5` 上拥有备份，但是节点 `1` 中的备份正在复制的过程中，本次查询不可用

---

## 文件存储路径
数据文件存储在 `$MILVUS_ROOT` 路径下
- `fragment` 数据文件: `/<user_name>/<collection_name>/data/<fragment_id>/<file_name>`
- `delete log` 数据文件: `/<user_name>/<collection_name>/data/<fragment_id>/<file_name>`

---

## 查询流程

```txt
                             查询流程

+--------+                             +---------+                             +---------+
| client |                             |  N1     |                             |  etcd   |
|        |     (1) Query q1            |         |  (2) request meta of T0     |         |
|        |----------------------------->         |----------------------------->         |
|        |                             |         |                             |         |
|        |                             | (F1 F2) |  (3) meta of T0             |         |
|        |                             |         <-----------------------------|         |
|        |                             |         |                             +---------+
|        |                             |         |  (4) q11 on F1
|        |                             |         |------------------+
|        |                             |         |                  |
|        |                             |         <------------------+
|        |                             |         |                              +---------+
|        |                             |         |  (4) q12 on F0               |   N2    |
|        |                             |         |------------------------------>         |
|        |                             |         |                              | (F0 F1) |
|        |                             |         |                              +---------+
|        |  (6) Response of q1         |         |
|        <-----------------------------|         |                              +---------+
|        |                             |         |  (4) q13 on F2               |   N3    |
|        |                             |         |------------------------------>         |
|        |                             |         |                              | (F0 F2) |
|        |                             |         |                              +---------+
|        |                             |         |  (5) reduce on q11 q12 q13
|        |                             |         |----------------------------+
|        |                             |         |                            |
|        |                             |         <----------------------------+
|        |                             |         |
+--------+                             +---------+
```

- `Milvus` 节点 `N1` 收到针对 `collection` `T0` 的查询指令 `q1`
- `N1` 从 `etcd` 获得当前的 `revision` 值 `r1`
- `N1` 提取 `T0` 在 `r1` 下的所有 `meta` 信息
- 根据 `meta` 信息，`N1` 得到这些信息
  - `T0` 一共包含 `3` 个`fragment`：`F0,F1,F2`
  - `F0` 位于 `N2 N3` 节点
  - `F1` 位于 `N1 N2` 节点
  - `F2` 位于 `N1 N3` 节点
- `N1` 向 `N1, N2, N3` 发送请求，获得节点当时的负载状态, 据此，`N1` 做出如下安排:
  - 向 `N1` 发送查询请求 `q11`，明确要求在文件 `F1` 上执行 `q1` 查询
  - 向 `N2` 发送查询请求 `q12`，明确要求在文件 `F0` 上执行 `q1` 查询
  - 向 `N3` 发送查询请求 `q13`，明确要求在文件 `F2` 上执行 `q1` 查询
- `N1` 收到 `q11 q12 q13` 结果后，执行 `reduce` 操作
- `N1` 向客户端发送查询过结果


**异常处理**
- 查询过程中，如果 `N1` 宕机，如何处理？
  - 因为 `client` 值直接与 `N1` 链接的
  - 那么 `client` 可以感知到与 `N1` 的链接断开，直接返回本次查询失败
- 查询过程中, 如果 `N2` 宕机，如何处理？
  - `N2` 宕机，`N1` 可以感知到 `q12` 查询失败
  -  根据 `meta` 可以知道 `q12` 负责的数据 `F0` 在 `N3` 上存在 `replica`
  - 向 `N3` 发送查询请求 `q14`，明确要求在文件 `F0` 上执行 `q1` 查询
- 如果 `N2 N3` 都宕机，那么 `N1` 无法获得本次查询需要的所有数据，直接放回查询失败，在 `N2 N3` 节点启动前，服务不可用
- `q11 q12 q13` 必须设置超时机制
- `meta` 信息显示 `F0` 正在往 `N2` 复制，此时如何处理？
  - 调度器选择已经存在 `replica` 的节点，不会选择正在复制的节点
  - 如果 `F0` 在所有存活节点上的状态都是正在复制，那么当前服务不可用

---


## 数据两阶段插入流程

```txt
                                     两阶段插入

+--------+                                   +--------+
|        |                                   |        |                                 +------+
|        |         (1) Insert(vector)        |        |      (3) Put("tmp-file")        |      |
|        |----------------------------------->        |---------------------------------> disk |
|        |                                   |        |                                 |      |
|        |         (2) Flush                 |        |                                 +------+
|        |----------------------------------->        |
|        |                                   |        |
|        |         (4) "tmp-etcd"            |        |                                 +------+
| client <-----------------------------------| Milvus |      (6) Put("tmp-etcd")        |      |
|        |                                   |        |---------------------------------> etcd |
|        |         (5) "tmp-etcd"            |        |                                 |      |
|        |----------------------------------->        |                                 +------+
|        |                                   |        |
|        |         (7) Success               |        |
|        <-----------------------------------|        |
|        |                                   |        |
|        |                                   |        |
+--------+                                   +--------+
```

- `client` 执行 `flush` 操作触发数据写磁盘
- `Milvus` 节点将数据写到磁盘，假设文件名为 `tmp-file`
- 获得 `tmp-file` 对应 `etcd` 中的 `key`，假设为 `tmp-etcd`
- 向 `client` 发送 `tmp-etcd` 字符串
- `client` 向 `milvus` 返回 `tmp-etcd` 字符串确认收到
- `milvus` 收到 `client`的返回后，再 `etcd` 插入 `tmp-etcd` -> `tmp-file`
- 向 `client` 发送 `flush` 操作成功
- 如果 `Milvus` 超时未收到 `client` 的返回，则删除 `tmp-file` 文件，切直接切断当前与 `client` 的链接

**注意事项**
`client`有下列`3`种状态
- `client` 收到 `flush` 操作成功或失败
- `client` 未收到任何信息，直接超时，表示当前操作失败
- `client` 收到 `tmp-etcd` 信息，当时未收到 `flush` 操作成功的信息，此时 `client` 需要重新链接并向 `milvus` 查询 `tmp-etcd` 是否在 `etcd` 中
- 步骤`(4)`和步骤`(5)`应该被包含在 `Flush`的 `SDK` 内部，不对 `client` 暴露，`client`调用 `Flush`之后处于阻塞状态，到步骤`(7)` 才返回  

---

## 插入流程
```txt
                                   插入流程

+--------+                         +--------+
| client |                         |  N1    |                                 +------+
|        |   (1) Insert(F0)        |        |      (2) Put(F0)                |      |
|        |------------------------->        |---------------------------------> disk |
|        |                         |        |                                 |      |
|        |                         |        |                                 +------+
|        |                         |        |
|        |                         |        |                                 +-------------+
|        |                         |        |      (3) Query State            |             |
|        |                         |        |---------------------------------> N2 N3 N4 N5 |
|        |                         |        |                                 |             |
|        |  (6) Insert Success     |        |                                 +-------------+
|        <-------------------------|        |
|        |                         |        |                                 +-------+
|        |                         |        |     (4) Copy F0 from N1         |       |
|        |                         |        |--------------------------------->   N3  |
|        |                         |        |                                 |       |
|        |                         |        |                                 +-------+
|        |                         |        |
|        |                         |        |                                 +------+
|        |                         |        |     (5) Put meta of F0          |      |
|        |                         |        |---------------------------------> etcd |
|        |                         |        |    replicas: [1, 3, -5]         |      |
|        |                         |        |                                 +------+
|        |                         |        |
|        |                         |        |                                 +-------+
|        |                         |        |     (7) Copy F0 from N1 or N3   |       |
|        |                         |        |--------------------------------->  N5   |
|        |                         |        |                                 |       |
|        |                         |        |                                 +-------+
+--------+                         +--------+
```

- `client` 向 `Milvus` 节点 `N1` 插入数据，生成`fragment` `F0`
- `N1` 将 `F0` 写入磁盘
- `N1` 向其它节点 `N2 ~ N5` 发送查询请求，询问其负载状态
- 根据 `N2 ~ N5` 的负载状态，及配置文件中 `num_replicas`，`N1` 决定将 `F0` 分别复制到 `N3` 和 `N5`
- 配置文件中 `sync_level` 为 1，要求 `F0` 至少被复制到一个节点上，才能向客户端返回
- `N1` 上 `N3` 发送复制指令，将文件 `F0`复制到 `N3`
- 向 `etcd` 中插入 `meta` 信息，其中 `replicas` 值为 `[1,3,-5]`；`1 3`表明`F0` 已经在 `N1` `N3` 节点上， `-5` 表明数据 `F0` 正在往 `N5` 复制的过程中
- 向 `client` 返回结果，表明插入成功
- 向 `N5` 发送复制指令，将文件 `F0` 从 `N1` 节点复制到 `N5` 节点


**注意事项**
- 采用批处理模式，保证 `flush` 内的操作整理成功或整体失败，不存在 `F0` 部分数据插入成功，部分数据插入失败
- 如果 `N1` 已经向 `client`返回插入成功，但是还未向 `N2` 和 `N5` 发送复制指令，此时 `N1` 宕机了，如何处理？
  - `N1` 节点内的定时任务可以启动复制操作
  - 如果某次查询用到了 `F0` 数据，查询指令也可以启动复制任务
- 插入请求中包含当前插入的数据量，以 `Byte` 计算
- 插入操作不做主键唯一性检查(待讨论)
- 同一个 `flush` 块内，插入操作必须属于同一个 `Collection`
  - 假设 `flush` 语句块为 : `{insert v1 into t1; insert v2 into t2; insert v3 into t1}`
  - 因为这个 `flush` 语句块同时向 `t1` 和 `t2` 插入数据，所以本次 `flush` 操作失败

---

## 插入转发-1
```txt
                                插入转发

+--------+                         +--------+
| client |                         |  N1    |                                 +---------------+
|        |   (1) Insert(F0)        |        |   (2) Query State               |               |
|        |------------------------->        |--------------------------------->  N2 N3 N4 N5  |
|        |                         |        |                                 |               |
|        |                         |        |                                 +---------------+
|        |   (5) Insert Success    |        |
|        <-------------------------|        |                                 +------+
|        |                         |        |   (3) Insert(F0)                |  N2  |
|        |                         |        |--------------------------------->      |
|        |                         |        |                                 |      |
|        |                         |        |   (4) Insert Success            |      |
|        |                         |        <---------------------------------|      |
|        |                         |        |                                 |      |
+--------+                         +--------+                                 +------+
```

- `client` 向 `Milvus` 节点 `N1` 插入数据 `F0`
- `N1` 发现自身没有富余的空间用于存放 `F0`
- `N1` 向其它节点 `N2 ~ N5` 发送查询请求，询问其负载状态
- 根据 `N2 ~ N5` 的负载状态, `N1` 决定将 `F0` 转发到 `N2` 节点
- `N1` 连接 `N2`，插入数据 `F0`
  - 对 `N2` 来说，`N1` 为 `client`
  - `N1` 向 `N2` 插入数据与前述的 `插入流程` 完全一致
- `N2` 向 `N1` 返回结果，表明插入成功
- `N1` 向 `client` 返回结果，表明插入成功

**注意事项**
- 当前节点的已经没有富余空间，需要将插入数据转发到其它节点上
- 如果所有节点均没有富余空间，转发到空闲空间最多的节点上
- 如果所有节点均没有空间能够容下本次插入的所有数据，则直接返回失败，不会把本次插入分成多份，然后插入到不同的节点上
- 插入转发与`Master`的转发不一样，`client` 不需要断开与 `N1`的链接
- 转发策略优化，以大块数据数为单位，数据尽可能均匀分布(待验证)
    - client 链接 N1
    - client 插入 1G 数据到 N1
    - client 依然链接 N1， 新插入 1G, N1 将这个新插入的 1G 数据转发到 N2 节点
    - client 依然链接　N1, 再插入 1G, N1 将这次插入的 1G 数据转发到 N3 节点
    - 以大数据块为单位，数据尽量均匀分布

---
## 插入转发-2
- 插入转发另一种实现模式
- 数据不经过 `N1` 转发
- `N1` 通知 `client` 断开链接，并且链接 `N2`
- `client` 直接将数据插入 `N2`

```
+--------+                                   +--------+
|        |                                   |        |
|        | (1) Memory/Disk usage reach 60%   |        |
|        <-----------------------------------|        |
|        |                                   |        |
|        | (2) N2 node IP and Port           |        |
|        <-----------------------------------|   N1   |
|        |                                   |        |
|        | (3) Close connection with N1      |        |
| client <----------------X------------------>        |
|        |                                   |        |
|        |                                   |        |
|        |                                   |        |
|        |                                   +--------+
|        |
|        |
|        |                                   +--------+
|        | (4) Connect with N2               |        |
|        |----------------------------------->   N2   |
|        |                                   |        |
+--------+                                   +--------+
```

---

## 复制 `replica` 流程
```txt
                          复制流程

+----+                          +----+
| N1 |                          | N3 |
|    | (1) Copy F0 from N1      |    | (2) Check if exist copy job of F0
|    |-------------------------->    |------------------------------------+
|    |                          |    |                                    |
|    |                          |    <------------------------------------+
|    |                          |    |
|    |                          |    | (3) Check if F0 exist on memory or disk
|    | (5) Copy F0 to N3        |    |-----------------------------------------+
|    |-------------------------->    |                                         |
|    |                          |    <-----------------------------------------+
|    |                          |    |
|    |                          |    |                                 +------+
|    |                          |    | (4) Get meta of F0              | etcd |
|    |                          |    <---------------------------------|      |
|    |                          |    |    replicas:[1,-3,-5]           |      |
|    |                          |    |                                 |      |
|    |                          |    | (6) Update meta of F0           |      |
|    |                          |    |--------------------------------->      |
|    |                          |    |   IF    replicas == [1,-3,-5]   |      |
|    |                          |    |   THEN  replicas == [1, 3,-5]   |      |
|    |                          |    |                                 |      | 
|    |                          |    |                                 +------+
+----+                          +----+
```
- 节点 `N1` 通知节点 `N3` 从节点 `N1` 复制数据 `F0`
- `N3` 检查当前的任务列表，没有存在复制数据 `F0` 的任务
- `N3` 检查当前节点的内存和磁盘，数据 `F0` 并不存在当前节点内
- `N3` 获得 `F0` 对应的 `meta`，其中 `replicas` 为 `[1,-3,-5]`
- `N3` 启动复制，从 `N1` 复制 数据 `F0`
- 使用 `CAS` 模式跟新 `F0` 的 `meta`，将 `replicas` 从 `[1,-3,-5]` 替换为 `[1,3,-5]`


**注意事项**
- 复制操作是`幂等`行为，即执行复制指令前，先检查数据是否在当前节点的内存或磁盘上
- 复制指令需要指明数据从哪个节点复制，如果未指明或源数据节点宕则随机选择存活的 `ReplicaNode`
- 使用 `CAS` 的模式更新 `meta`
- 如果 `N3` 的任务列表没有复制`F0` 的任务，`N3` 的内存及磁盘也不存在 `F0` 数据，但是 `replicas` 为 `[1,3,-5]`
  - 因为 `replicas` 为 `[1,3,-5]`，表明 `F0` 曾经复制到 `N3` 节点
  - 但是当前内存及磁盘并不存在 `F0` 数据
  - 那么唯一合理的解释就是因为内存不够用，文件`F0`被抛弃了，下次查询用到 `F0` 时，需要从 `S3` 加载
- `N3` 在复制 `F0` 的过程中，因为有新数据插入，导致空间不够，如何处理？
  - 复制失败，打印日志
  - 查询其它节点是否有富余的存储空间，比如 `N2`节点
  - 使用 `CAS` 模式跟新 `meta` 将 `replicas` 从 `[1,-3,-5]` 修改为 `[1,-2,-5]`
  - 通知 `N2` 节点启动复制，从 `N1` 节点复制 `F0`

---

## 删除流程
```txt
+--------+                               +----+
| client |                               | N1 |
|        | (1) del1 : delete v0 from T0  |    |                                        +------+
|        |------------------------------->    | (2) get meta of T0                     | etcd |
|        |                               |    <---------------------------------------->      |
|        |                               |    |                                        |      <-----+
|        |                               |    | (3) del11 : delete v0 from F1          |      |     |
|        |                               |    |-------------------------------+        +------+     |
|        |                               |    |                               |                     |
|        |                               |    <-------------------------------+                     |
|        |                               |    |                                  +----+             |
|        |                               |    | (3) del12 : delete v0 from F0    | N2 |             |
|        |                               |    |---------------------------------->    |             |
|        |                               |    |                                  |    |             |
|        | (7) Delete Success            |    |                                  +----+             |
|        <-------------------------------|    |                                                     |
|        |                               |    |                                  +----+             |
|        |                               |    | (3) del13 : delete v0 from F2    | N3 |             |
|        |                               |    |---------------------------------->    |             |
|        |                               |    |                                  |    |             |
|        |                               |    |                                  +----+             |
|        |                               |    |                                                     |
|        |                               |    |                                 +----+              |
|        |                               |    | (4) old delete log on F0        | S3 |              |
|        |                               |    <---------------------------------|    |              |
|        |                               |    |     kDoldF0->kVoldF0            |    |              |
|        |                               |    |                                 |    |              |
|        |                               |    |                                 |    |              |
|        |                               |    | (5) new delete log on F0        |    |              |
|        |                               |    |--------------------------------->    |              |
|        |                               |    |     kDnewF0->kVnewF0            |    |              |
|        |                               |    |                                 +----+              |
|        |                               |    |                                                     |
|        |                               |    |                                                     |
|        |                               |    | (6) put kDnewF0->kVnewF0, replicas:[1,-2,-3]        |
|        |                               |    |-----------------------------------------------------+
|        |                               |    |  IF value(kF0) = mF0 AND value(kDoldF0) = kVoldF0
|        |                               |    |
|        |                               |    |                                 +-------+
|        |                               |    | (8) copy new delete log from N1 | N2 N3 |
|        |                               |    |--------------------------------->       |
|        |                               |    |                                 |       |
+--------+                               +----+                                 +-------+
```
- `client` 向 `N1` 发送删除指令`del1`，要求从 `collection` 表 `T0` 中删除主键为 `v0` 的数据
- `N1` 从 `etcd` 获取最新 `revision` 记为 `r1`，并获得 `r1` 下 `T0` 的所有 `meta`
- 根据 `meta` 信息，`N1` 得到一下信息
  - `T0` 一共包含 `3` 个`fragment`：`F0,F1,F2`
  - `F0` 位于 `N2 N3` 节点，`meta` 对应的 `kv` 为 `kF0`->`mF0`
  - `F1` 位于 `N1 N2` 节点，`meta` 对应的 `kv` 为 `kF1`->`mF1`
  - `F2` 位于 `N1 N3` 节点，`meta` 对应的 `kv` 为 `kF2`->`mF2`
- `N1` 向 `N1, N2, N3` 发送请求，获得节点当时的负载状态, 据此，`N1` 做出如下安排:
  - 向 `N1` 发送查询请求 `del11`，在文件 `F1` 上查询 `v0` 的 `offset`
  - 向 `N2` 发送查询请求 `del12`，在文件 `F0` 上查询 `v0` 的 `offset`
  - 向 `N3` 发送查询请求 `del13`，在文件 `F2` 上查询 `v0` 的 `offset`
- `del11 del12 del13` 返回结果如下：
  - `del11` : -1
  - `del12` : Noffset
  - `del13` : -1
- `N1` 根据 `del11 del12 del13` 可以知，向量 `v0` 只在 `fragment` `F0`上
- 获得 `r1` 下 `F0` 的 `delete log`,假设对应的 `kv` 为 `kDoldF0` -> `kVoldF0`
- 假设 `kVoldF0` 对应的 `S3` 文件内容为 `{T0, vx, F0, Xoffset}`
- 生成新的 `delete log` 文件，内容如下
```txt
{T0, vx, F0, Xoffset}
{T0, v0, F0, Noffset}
```
- 假设新生成的 `delete log` `meat` 对应的 `kv` 为 `kDnewF0` -> `kVnewF0`
- 查询 `meta` 可以 `fragment` `F0` 当前位于节点 `N2 N3` 上,所以新生成的 `delete log` 也需要被复制到 `N2 N3` 上
- 又因为 当前的 `delete log` 在节点 `N1` 上生成，所以 `kVnewF0` 对应的 `replicas` 属性为 `[1,-2,-3]`，表示当前文件在 `N1` 生成，但是需要被复制到 `N2 N3`
- 采用两阶段提交的方式将 `kVnewF0` 写入磁盘 并采用 `CAS` 模式更新 `meta`:
  - `IF value(kF0) = mF0 AND value(kDoldF0) = kVoldF0`
  - `THEN put kDnewF0 kVnewF0`
- 向 `client` 发送删除操作 `del1` 执行成功 
- 向 `N2 N3` 发送复制指令，将文件 `kVnewF0`对应的文件 从 `N1` 节点复制到 `N2 N3` 节点


**注意事项**
- 采用 `CAS` 模式更新 `meta` ，只有在保证以下两个条件下才能更新 `delete log` 的 `meta`
  - `/<user_name>/<collection_name>/data/<fragment_id>`没有发生改变，即在删除过程中 `delete log` 所属于的 `fragment` 没有发生改变
  -  `/<user_name>/<collection_name>/delete_log/<fragment_id>`没有发生改变，即删除过程中，没有别的用户在同一个 `fragment` 上删除数据
- 删除操作只生成 `delete log`，每条记录包含以下内容:
  - collection_name
  - 向量 `ID`
  - 向量所在的 `fragment id`
  - 向量被删除向量的行号索引，从0 开始
- 同一个 `flush` 块内必须为同一个`collection`的删除操作，不能混搭
  - 假设 `flush` 语句块为 : `{delete v1 from t1; delete v2 from t2;}`
  - 因为这个 `flush` 语句块同时从 `t1` 和 `t2` 删除数据，所以本次 `flush` 操作失败
- 和插入数据一样，删除数据更新 `meta` 也设计涉及到两阶段提交
- 根据 `old delte log` 生成 `new delete log` 后，并不直接从磁盘 及 `Milvus`节点的内存中删除 `old delete log`，应为该文件可能正在被某次查询使用

---

## 创建索引(合并文件)
- `N1` 节点触发了针对 `collection` `T0`  的创建索引任务
- `N1` 节点获得 `etcd` 最新的 `revision` 值 `r1`，并获得 `T0` 的最新 `meta`
- 根据 `meta`，`N1` 得知 `T0` 包含 `5` 个 `fragment` ： `{F0, F1, F2, F3, F4}`
- `{F1, F2, F3}` 的 `InsertNode` 为 `N1` 且满足合并条件，它们的布局如下，将这三个`fragment`合并，并创建向量索引
  - `F1` 包含原始向量文件 `A1` 和 `delete log` `D1`
  - `F2` 包含索引文件 `I2`，`I2` 由原始向量文件 `A2` 创建，并且没有 `delete log`
  - `F3` 包含索引文件 `I3`，`I3` 由原始向量文件 `A31 A32` 创建，并且拥有delete log `D3`
```txt
合并前 fragment 布局


+-----------------------------------------+
| fragment (F1)                           |
|  +---------------+ +-----------------+  |
|  | raw file (A1) | | delete log (D1) |  |
|  +---------------+ +-----------------+  |
+-----------------------------------------+

+-----------------------------------------+
| fragment (F2)                           |
|  +-----------------+ +---------------+  |
|  | index file (I2) | | raw file (A2) |  |
|  +-----------------+ +---------------+  |
+-----------------------------------------+

+--------------------------------------------------------------+
| fragment (F3)                                                |
|  +-----------------+ +----------------+ +-----------------+  |
|  | index file (I3) | | raw file (A31) | | delete log (D3) |  |
|  |                 | +----------------+ |                 |  |
|  |                 | +----------------+ |                 |  |
|  |                 | | raw file (A32) | |                 |  |
|  +-----------------+ +----------------+ +-----------------+  |
+--------------------------------------------------------------+
```
- 将 `A1 A2 A31 A32` 在内存中拼接生成一个文件 `A9`，并在 `I9` 建立索引文件 `I9`
- 将 `D1 D3` 合并生成一个新的 `delete log` `D9`
```txt
合并后 fragment 布局


+--------------------------------------------------------------+
| fragment (F9)                                                |
|  +-----------------+ +----------------+ +-----------------+  |
|  | index file (I9) | | raw file (A1)  | | delete log (D9) |  |
|  |                 | +----------------+ |                 |  |
|  |                 | +----------------+ |                 |  |
|  |                 | | raw file (A2)  | |                 |  |
|  |                 | +----------------+ |                 |  |
|  |                 | | raw file (A31) | |                 |  |
|  |                 | +----------------+ |                 |  |
|  |                 | +----------------+ |                 |  |
|  |                 | | raw file (A32) | |                 |  |
|  +-----------------+ +----------------+ +-----------------+  |
+--------------------------------------------------------------+
```
- `N1` 负责将 `I9 D9` 写入磁盘
- `N1` 向其它节点 `N2 ~ N5` 发送查询请求，询问其负载状态
- 根据 `N2 ~ N5` 的负载状态，及配置文件中 `num_replicas`，`N1` 决定将 `I9 D9` 分别复制到 `N3` 和 `N5`
- 以事务的方式向 `etcd` 插入两条 `meta`:
  - `fragment F9` 对应的 `meta`，其中 `replicas` 值为 `[1,-3,-5]`
  - `delete log D9` 对应的 `meata`，其中 `replicas` 值为 `[1,-3,-5]`
- 向 `N3 N5` 发送复制指令，将文件 `I9 D9` 从 `N1` 节点复制到 `N3 N5` 节点


**注意事项**
- 由原始向量文件的 `InsertNode` 负责创建该文件的索引
- 按照 [逻辑存储格式](##逻辑存储格式) `fragment 2` 所示，创建索引的过程中伴随这文件合操作
  - 原始向量文件是`逻辑`意义上合并，但是`delete log` 则是物理意义上的合并
  - `raw file 1, raw file 2, raw file 3`在逻辑意义上合并成一个文件，并依据这个合并的文件创建了索引文件 `Index file`
  - `raw file 1, raw file 2, raw file 3`必须属于同一个 `InsertNode`
  - `delete log 1, delete log 2, delete log 3` 在物理意义上合并成一个新的文件 `delete log`
  - `delete log 1, delete log 2, delete log 3` 并不关心是否属于同一个 `InsertNode`；但是 `delete log 1` 必须和 `raw file 1` 属于同一个 `fragment`, `delete log 2` 必须和 `raw file 2` 属于同一个 `fragment`, `delete log 3` 必须和 `raw file 3` 属于同一个 `fragment`
  - 合并操作中，并不要求输入的向量文件为原始向量文件，也可以是小数据集合的索引文件，即将多个小数据集的索引文件合并成一个大的索引文件
- 创建索引由 `Milvus` 节点自动发起
- 采用 `CAS` 模式更新 `meta` ，只有在保证以下两个条件下才能更新索引文件 的 `meta`
  - `/<user_name>/<collection_name>/data/<fragment_id>`没有发生改变，即在创建索引的过程中，原始向量文件没有发生改变；如果是多个原始向量文件合并成索引文件，则必须保证相关的原始向量文件未发生改变
  - `/<user_name>/<collection_name>/delete_log/<fragment_id>`没有发生改变，如果伴随这 `delete log` 的合并操作，那么必须保证在创建索引的过程中相关的 `delete log` 未发生修改
- 文件合并之后，`D1 D3 I2 I3`不会被后续查询使用到，但是可能被当前正在进行的某个查询使用，因此这些文件不会直接删除，而是有由 `Miluvs` 后台程序定时扫描无用的文件，然后负责删除


---

## `Milvus`节点重启后需要加载的数据
- `meta` 的 `replicas` 属性指定了当前 `fragment` 和 `delete log` 被哪个 `Milvus` 节点加载
- 根据 `meta` 信息，从其它节点复制属于本节点的 `fragment` 和 `delete log`
- 如果当前 `Milvus` 节点的内存放不下所有的 `fragment` ，则按照 `collection 创建时间`,`fragment 创建时间` 两级排序，优先将最近的数据加载到内，其余数据缓存到 `Milvus` 节点的本地磁盘；如果磁盘也放不下，则不加载后续数据

---

## 如何处理节点宕机
- `Master` 定期向 `Milvus` 节点发送心跳信号请求，节点 `N1` 连续 `time_out_on_heart_beat` 次心跳信号超时，认为节点 `N1` 宕机
- `N1` 宕机后，`InsertNode` 为 `N1` 的 `fragment` 无法创建索引，也无法合并；`Master` 负责修改这些 `fragment` 的 `InsertNode`
- 采用 `CAS` 模式修改 `meta` 的 `replicas`
  - 因为 `创建索引(合并文件)` 只能由 `InsertNode` 发起，所以需要 `replicas` 修改成非 `N1` 的其它节点
  - 因为 `N1` 上的数据已经不可用，所以 `replicas` 属性中所有 `1` 需要改成 `-1` ，表示数据正在复制 

---

## 动态扩容量
- 动态扩容不影响已经存在文件分布，只有后续新插入的数据能够进入新增节点
- 动态扩容可以保证服务不暂停
- 只支持动态扩容，不支持动态缩容

---

## 静态扩缩容
- 静态扩容需要暂停 `Milvus-Cluster` 服务
- 根据 `Milvus-Cluster` 集群的整体配置，修改 `meta` 的 `replicas` 属性，再重启所有的 `Milvus` 节点

---

## 动态优化
- 优化过程中，服务器不暂停，`Client` 客户端无感知
- 不是由 `Milvus-Cluster` 自动发起的，而是由管理员手动触发的
- 配合扩缩容使用，动态的修改数据在节点上的分布
- 主要包含亮方面的修改
  - 修改配置文件的 `num_replicas`
  - 修改 `meta` 的 `replicas` 属性

---

## 关于之前人通提出的几个问题及解决方案
- 问题1：
  - 本设计认为当前 `Milvus` 的 `WAL` 语义模糊，应该删除
- 问题2:
  - 本设计没有 `primary key` 和 `fragment` 的映射关系，所以删除数据也是全表扫描的
- 问题3:
  - 本设计暂时不考虑主键唯一性检查
- 问题4：
  - 根据[前提假设](##前提假设)，同一个客户端严格保操作时序，不同客户端不保证时序
- 问题5：
  - 和问题4一样,同一个客户端严格保操作时序，不同客户端不保证时序
- 问题6：
  - 设置 `auto_flush` 后，`insert` 和 `delete` 操作才是异步的，此时出现错误，只做日志记录，不向 `client` 返回任何信息
  - 未设置 `auto_flush`，则所有操作均为同步
- 问题7：
  - `etcd` 性能测试报告: https://github.com/adream307/notes/blob/master/etcd-cluster/performance_test.md
    - 底库有 `100万` 条记录，每条记录的 `key` 包含 `0.5k` 数据，`value` 包含 `1k` 数据
    - `100` 客户端同时链接，每个客户端每次请求`1000`条记录, 共发送 `1万`次数据请求
    - 总共耗时 `41m14.953949382s`，相当于每次请求耗时 `0.002474953949382s`
